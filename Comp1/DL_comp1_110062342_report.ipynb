{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Deep Learning Competition 1: Text Feature Engineering</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By 110062342 楊博聖"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">1. Data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "讀入Training Data與Testin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.output { max-height: 1000px; overflow-y: auto; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.output { max-height: 1000px; overflow-y: auto; }</style>\"))\n",
    "df_train = pd.read_csv('./dataset/train.csv')\n",
    "df_test = pd.read_csv('./dataset/test.csv')\n",
    "# print(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">2. Preprocess: Data Cleaning</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先檢視一下沒處理過的data中有什麼內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><head><div class=\"article-info\"> <span class=\"byline basic\">Clara Moskowitz</span> for <a href=\"/publishers/space-com/\">Space.com</a> <time datetime=\"Wed, 19 Jun 2013 15:04:30 +0000\">2013-06-19 15:04:30 UTC</time> </div></head><body><h1 class=\"title\">NASA's Grand Challenge: Stop Asteroids From Destroying Earth</h1><figure class=\"article-image\"><img class=\"microcontent\" data-fragment=\"lead-image\" data-image=\"http://i.amz.mshcdn.com/I7b9cUsPSztew7r1WT6_iBLjflo=/950x534/2013%2F06%2F19%2Ffe%2FDactyl.44419.jpg\" data-micro=\"1\" data-url=\"http://mashable.com/2013/06/19/nasa-grand-challenge-asteroid/\" src=\"http://i.amz.mshcdn.com/I7b9cUsPSztew7r1WT6_iBLjflo=/950x534/2013%2F06%2F19%2Ffe%2FDactyl.44419.jpg\"/></figure><article data-channel=\"world\"><section class=\"article-content\"> <p>There may be killer asteroids headed for Earth, and NASA has decided to do something about it. The space agency announced a new \"Grand Challenge\" on June 18 to find all dangerous space rocks and figure out how to stop them from destroying our planet.</p> <p>The new mission builds on projects already underway at NASA, including a plan to <a href=\"http://www.space.com/20591-nasa-asteroid-capture-mission-feasibility.html\" target=\"_blank\">capture an asteroid</a>, pull it in toward the moon and send astronauts to visit it. As part of the Grand Challenge, the agency issued a \"request for information\" today — aiming to solicit ideas from industry, academia and the public on how to improve the asteroid mission plan.</p> <p>\"We're asking for you to think about concepts and different approaches for what we've described here,\" William Gerstenmaier, NASA's associate administrator for human explorations and operations, said yesterday during a NASA event announcing the initiative. \"We want you to think about other ways of enhancing this to get the most out of it.\"</p> <p><divclass><strong>SEE ALSO: <a href=\"http://www.space.com/20606-nasa-asteroid-capture-mission-images.html\" target=\"_blank\">How It Works: NASA Asteroid-Capture</a></strong><br><br>Responses to the request for information, which also seeks ideas for detecting and mitigating asteroid threats, are due July 18.<br><br>The asteroid-retrieval mission, designed to provide the first deep-space mission for astronauts flying on NASA's Space Launch System rocket and Orion space capsule under development, has come under fire from lawmakers who would prefer that NASA return to the moon.<br><br>A <a href=\"http://www.space.com/21609-nasa-asteroid-capture-mission-congress.html\" target=\"_blank\">draft NASA authorization bill</a> from the House space subcommittee, which is currently in debate, would cancel the mission and steer the agency toward other projects. That bill will be discussed during a hearing Wednesday, June 19 at 10 a.m. EDT.<br><br><divclass><strong>SEE ALSO: <a href=\"http://www.space.com/20606-nasa-asteroid-capture-mission-images.html\" target=\"_blank\">How It Works: NASA Asteroid-Capture Mission in Pictures</a></strong><br><br>But NASA officials defended the asteroid mission today and said they were confident they'd win Congress' support once they explained its benefits further.<br><br>\"I think that we really, truly are going to be able to show the value of the mission,\" NASA Associate Administrator Lori Garver said today. \"To me, this is something that what we do in this country — we debate how we spend the public's money. This is the beginning of the debate.\"<br><br>Garver also maintained that sending astronauts to an asteroid would not diminish NASA's other science and exploration goals, including another lunar landing.<br><br><divclass><strong>SEE ALSO: <a href=\"http://www.space.com/20601-animation-of-proposed-asteroid-retrieval-mission-video.html\" target=\"_blank\">Animation Of Proposed Asteroid Retrieval Mission</a></strong><br><br>\"This initiative takes nothing from the other valuable work,\" she said. \"This is only a small piece of our overall strategy, but it is an integral piece. It takes nothing from the moon.\"<br><br>Part of NASA's plan to win support for the flight is to link it more closely with the larger goal of protecting Earth from asteroid threats.<br><br>If, someday, humanity discovers an asteroid headed for Earth and manages to alter its course, \"it will be one of the most important accomplishments in human history,\" said Tom Kalil, deputy director for technology and innovation at the White House Office of Science and Technology Policy.<br><br><divclass><strong>SEE ALSO: <a href=\"http://www.space.com/20006-deep-space-missions-private-companies.html\" target=\"_blank\">Wildest Private Deep-Space Mission Ideas: A Countdown</a></strong><br><br>The topic of asteroid threats is more timely than ever, after a meteor exploded over the Russian city of <a href=\"http://www.space.com/19823-russia-meteor-explosion-complete-coverage.html\" target=\"_blank\">Chelyabinsk</a> on Feb. 15 — the same day that the football field-sized <a href=\"http://www.space.com/19646-asteroid-2012-da14-earth-flyby-complete-coverage.html\" target=\"_blank\">asteroid 2012 DA14</a> passed within the moon's orbit of Earth.<br><br><em>Image courtesy of <a href=\"http://www.dvidshub.net/image/707596/ida-and-dactyl#.UcHDQvk4uSo\" target=\"_blank\">NASA</a></em></br></br></br></br></divclass></br></br></br></br></br></br></br></br></divclass></br></br></br></br></br></br></br></br></divclass></br></br></br></br></br></br></br></br></divclass></p> <ul> <li><a href=\"http://www.space.com/34406-spacexs-musk-says-sabotage-unlikely-cause-of-sept-1-explosion-but-still-a-worry.html\">SpaceX's Musk Says Sabotage Unlikely Cause of Sept. 1 Explosion, But Still a Worry</a></li> <li><a href=\"http://www.space.com/34405-proxima-centauri-starspots-stellar-cycle-habitable-planet-alien-life.html\">Proxima Centauri Is Like Our Sun... on Steroids</a></li> <li><a href=\"http://www.space.com/34404-china-launches-shenzhou-11-astronauts-to-space-lab.html\">China Launches Shenzhou-11 Astronauts to Tiangong-2 Space Lab</a></li> <li><a href=\"http://www.space.com/34403-space-station-mockup-in-houston-astronaut-guided-tour-video.html\">Space Station Mockup In Houston - Astronaut Guided Tour | Video</a></li> </ul> <p> This article originally published at Space.com <a href=\"http://www.space.com/21610-nasa-asteroid-threat-grand-challenge.html?\">here</a> </p> </section></article><footer class=\"article-topics\"> Topics: <a href=\"/category/asteroid/\">Asteroid</a>, <a href=\"/category/asteroids/\">Asteroids</a>, <a href=\"/category/challenge/\">challenge</a>, <a href=\"/category/earth/\">Earth</a>, <a href=\"/category/space/\">Space</a>, <a href=\"/category/us/\">U.S.</a>, <a href=\"/category/world/\">World</a> </footer></body></html>\n"
     ]
    }
   ],
   "source": [
    "print(df_train.loc[0, 'Page content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到沒處理過的data非常亂，有不少的html、XML內容，但是這些tag也提供了很適合用來分割資料的資訊。\n",
    "\n",
    "這裡使用助教在網頁教我們的BeautifulSoup來進行Data cleaning，以下是我這次Extract的Features:\n",
    "\n",
    "&nbsp; -title: 文章標題 <br>\n",
    "&nbsp; -title_len: 文章長度 <br>\n",
    "&nbsp; -author: 作者(去除by/By，並將其全部換成小寫)<br>\n",
    "&nbsp; -channel: 頻道<br>\n",
    "&nbsp; -topic: 文章的章節主題<br>\n",
    "&nbsp; -Day and Date: 時間，包含日期(年、月、日)以及星期幾，若文章沒有這個資訊則使用Fri, 11 Oct 2013 22:10:39替代，輸出結果為weekday, year, month, day<br>\n",
    "&nbsp; -Content_len: 文章長度<br>\n",
    "&nbsp; -image_num: 圖片數量<br>\n",
    "&nbsp; -link_num: 連結數量<br>\n",
    "\n",
    "經過多次的實驗與嘗試，發現到取用author, topic, weekday, year, month, day, content_len這些資訊最為有效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def preprocessor(text):\n",
    "    \n",
    "    # remove HTML tags\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "    # title\n",
    "    title_tag = soup.body.h1\n",
    "    title = title_tag.string.strip().lower() if title_tag else ''\n",
    "    \n",
    "    # title len\n",
    "    title_len = len(title_tag.get_text().split()) if title_tag else 0\n",
    "\n",
    "\n",
    "    # author\n",
    "    article_info = soup.head.find('div', {'class': 'article-info'})\n",
    "    author_name = article_info.find('span', {'class': 'author_name'})\n",
    "    if author_name != None:\n",
    "        author = author_name.get_text()\n",
    "    elif article_info.span != None:\n",
    "        author = article_info.span.string\n",
    "    else:\n",
    "        author = article_info.a.string\n",
    "\n",
    "    author = author.lower()\n",
    "\n",
    "    if author.startswith('by '):\n",
    "        author = author[3:]\n",
    "    author = re.sub('&.*;', '&', author.replace(' and ', ' & '))\n",
    "\n",
    "    # channel\n",
    "    channel = soup.body.article['data-channel'].strip().lower()\n",
    "\n",
    "    # topic\n",
    "    topic_element = soup.find(attrs={'class': 'article-topics'})\n",
    "    topic = topic_element.get_text().replace('Topics', '').replace(':', '').replace(',', '').strip().lower() if topic_element else ''\n",
    "\n",
    "    # day and date\n",
    "    article_info = soup.head.find('div', {'class': 'article-info'})\n",
    "    try:\n",
    "        date_time = article_info.time['datetime']\n",
    "    except:\n",
    "        date_time = 'Fri, 11 Oct 2013 22:10:39'\n",
    "    match_obj = re.search('([\\w]+),\\s+([\\d]+)\\s+([\\w]+)\\s+([\\d]+)\\s+([\\d]+):([\\d]+):([\\d]+)', date_time)\n",
    "    weekday, day, month, year, hour, minute, second = match_obj.groups()\n",
    "    weekday, month = weekday.lower(), month.lower()\n",
    "    \n",
    "    # len of content\n",
    "    content = soup.body.find('section', {'class': 'article-content'}).get_text()\n",
    "    content_len = len(content)\n",
    "\n",
    "    # image num\n",
    "    image_num = len(soup.body.find_all('img'))\n",
    "\n",
    "    # link num\n",
    "    link_num = len(soup.body.find_all('a'))\n",
    "\n",
    "    return author, topic, weekday, year, month, day, content_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著要針對一些文字進行處理，包含星期幾要轉換成數字，月份也要轉換成數字。最後可以看到處理完的資料如下面所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train data:   0%|          | 0/27643 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train data: 100%|██████████| 27643/27643 [01:52<00:00, 246.59it/s]\n",
      "Processing test data: 100%|██████████| 11847/11847 [00:53<00:00, 220.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Author                                              Topic  \\\n",
      "0   clara moskowitz  asteroid asteroids challenge earth space u.s. ...   \n",
      "1  christina warren  apps and software google open source opn pledg...   \n",
      "2         sam laird      entertainment nfl nfl draft sports television   \n",
      "3         sam laird                    sports video videos watercooler   \n",
      "4   connor finnegan  entertainment instagram instagram video nfl sp...   \n",
      "\n",
      "   Weekday  year  month day  Length of Content  \n",
      "0        3  2013      6  19               3591  \n",
      "1        4  2013      3  28               1843  \n",
      "2        3  2014      5  07               6646  \n",
      "3        5  2013     10  11               1821  \n",
      "4        4  2014      4  17               8919  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "feature_list = []\n",
    "\n",
    "# Processing training data with tqdm progress bar\n",
    "for text in tqdm(df_train['Page content'], desc=\"Processing train data\"):\n",
    "    feature_list.append(preprocessor(text))\n",
    "\n",
    "# Processing test data with tqdm progress bar\n",
    "for text in tqdm(df_test['Page content'], desc=\"Processing test data\"):\n",
    "    feature_list.append(preprocessor(text))\n",
    "\n",
    "df_extract = pd.DataFrame(\n",
    "    feature_list,\n",
    "    columns=['Author', 'Topic', 'Weekday', 'year', 'month', 'day','Length of Content']\n",
    ")\n",
    "\n",
    "day = {'mon': 1, 'tue': 2, 'wed': 3,'thu': 4, 'fri': 5, 'sat': 6, 'sun': 7}\n",
    "\n",
    "month = {'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, 'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12}\n",
    "\n",
    "df_final = df_extract.copy()\n",
    "df_final['Weekday'] = df_final['Weekday'].map(day)\n",
    "df_final['month'] = df_final['month'].map(month)\n",
    "\n",
    "print(df_final.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">3. Preprocess: Word Stemming and Stop-word Removal</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Word Stemming <br>\n",
    "使用 PorterStemmer 進行詞幹提取。\n",
    "將每個單字還原成原型，例如將 \"running\" 還原為 \"run\"，以減少單詞的變化形式對分析的影響。\n",
    "\n",
    "2. Stop-word Removal <br>\n",
    "從 NLTK 的停用詞列表中加載英文停用詞。在文本分詞後，去除這些常見但意義不大的詞，如 “the”, “is”, “in” 等，以提升文本處理的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/stylish/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/stylish/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/stylish/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    if type(text) == np.ndarray:\n",
    "        text = text[0]\n",
    "    return re.split('\\s+', text.strip())\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "\n",
    "    if type(text) == np.ndarray:\n",
    "        text = text[0]\n",
    "        \n",
    "    text = re.sub(\"([\\w]+)'[\\w]+\", (lambda match_obj: match_obj.group(1)), text)\n",
    "    text = re.sub('\\.', '', text)\n",
    "    text = re.sub('[^\\w]+', ' ', text)\n",
    "    porter= PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">4. BoW(Bag of Words)<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用助教於網站上提供的BoW完成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# title removed\n",
    "trans_author_topic_title = ColumnTransformer(\n",
    "    [('Author', 'drop', [0]),\n",
    "     ('Topic', CountVectorizer(tokenizer=tokenizer_stem_nostop, lowercase=False), [1])],\n",
    "    n_jobs=-1,\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">5. Data Split(8:2)<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將資料先分回Train與Test，再將Train的資料以8:2進行切割，並印出前五項確認資料的形式無誤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['seth fiegerman'\n",
      "  'apple business gadgets ipad ipad air ipad mini stocks' 5 '2013' 11\n",
      "  '01' 1567]\n",
      " ['jason abbruzzese' 'business media vice media' 4 '2014' 9 '04' 2101]\n",
      " ['brian womack' 'advertising business mobile twitter' 3 '2013' 12 '25'\n",
      "  1812]\n",
      " ['tricia gilbride'\n",
      "  'coffee john oliver last week tonight with john oliver pumpkin television video videos viral video watercooler'\n",
      "  1 '2014' 10 '13' 695]\n",
      " ['emily banks' 'ces tech' 6 '2013' 1 '12' 5737]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train_dataset = df_final.values[:df_train.shape[0]]\n",
    "y_train_dataset = (df_train['Popularity'].values == 1).astype(int)\n",
    "X_test_dataset = df_final.values[df_train.shape[0]:]\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_dataset, y_train_dataset, test_size=0.2, random_state=0)\n",
    "print(X_train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">6. Model Training<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用一個簡單的Training函數，透過傳入classifier與其名字分別現在在訓練的是哪一個classifier，並使用roc_auc_score判斷其訓練結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def training(classifier_name, classifier):\n",
    "    classifier.fit(X_train, y_train)\n",
    "    print(f'Classifier Name: {classifier_name}, train score: {roc_auc_score(y_train, classifier.predict_proba(X_train)[:, 1]):.4f}')\n",
    "    print(f'Classifier Name: {classifier_name}, valid score: {roc_auc_score(y_valid, classifier.predict_proba(X_valid)[:, 1]):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">7-1. Model: Random Forest Classifier<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stylish/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Name: Random Forest Classifier, train score: 1.0000\n",
      "Classifier Name: Random Forest Classifier, valid score: 0.5867\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "RF = Pipeline([('ct', trans_author_topic_title),\n",
    "                 ('clf', RandomForestClassifier(n_jobs=-1, random_state=0, n_estimators=500))])\n",
    "\n",
    "training(\"Random Forest Classifier\", RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">7-2. Model: LGBM<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stylish/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 10885, number of negative: 11229\n",
      "[LightGBM] [Info] Total Bins 2740\n",
      "[LightGBM] [Info] Number of data points in the train set: 22114, number of used features: 824\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492222 -> initscore=-0.031114\n",
      "[LightGBM] [Info] Start training from score -0.031114\n",
      "Classifier Name: Light Gradient Booster Machine Classifier, train score: 0.6786\n",
      "Classifier Name: Light Gradient Booster Machine Classifier, valid score: 0.5946\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "LGBM = Pipeline([('ct', trans_author_topic_title),\n",
    "                 ('clf', LGBMClassifier(force_row_wise=True, random_state=0, learning_rate=0.009, n_estimators=400))])\n",
    "\n",
    "training(\"Light Gradient Booster Machine Classifier\", LGBM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">8. Prediction<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在多次的參數更改與實驗下，與最後投到Kaggle的成績來看，是LGBM Classifier的表現最好，便選用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = LGBM.predict_proba(X_test_dataset)[:, 1]\n",
    "df_pred = pd.DataFrame({'Id': df_test['Id'], 'Popularity': y_score})\n",
    "df_pred.to_csv('prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">9. Conclusion<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這次的lab是我第一次做有關Text feature engineering的內容，一開始還很怕什麼都做不出來，但很感謝助教提供那麼詳細與豐富的教學網站讓我能夠有個基礎方向與理解。我學會了如何使用Beautiful soup去處理HTML的資料，也學會更多的Data cleaning方式，例如Word stemming、Stop-word Removal等等。最後也有成功找出適合的模型與參數進行訓練。這也是我第一次直接使用Package內建的Classifier做training，不同於以往需要自己將整個Model建立好，使用別人寫好的真的方便許多，只是也有諸多限制，像是我得遵照他們的參數設定去輸入，相比之下沒有那麼靈活。我也得自行查看每個classifier的功用與能力分別是什麼再去判斷要使用什麼。最終是選定兩種下來進行測試。在Kaggle的Result上，Public是42名，而Private則是提升到了31名，算是還不錯的成績。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
